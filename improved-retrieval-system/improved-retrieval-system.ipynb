{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IR Lab SoSe 2024: Baseline Retrieval System\n",
    "\n",
    "This jupyter notebook serves as baseline retrieval system that you can try to improve upon.\n",
    "We will use the a corpus of scientific papers (title + abstracts) from the fields of information retrieval and natural language processing (the [IR Anthology](https://ir.webis.de/anthology/) and the [ACL Anthology](https://aclanthology.org/)). This serves Jupyter notebook only serves as retrieval system, i.e., it gets a set of information needs (topics) and a corpus as input and produces a run file as output. Please do evaluations in a new dedicated notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Import Libraries\n",
    "\n",
    "We will use [tira](https://www.tira.io/), an information retrieval shared task platform, for loading the (pre-built) retrieval index and [ir_dataset](https://ir-datasets.com/) to subsequently build a retrieval system with [PyTerrier](https://github.com/terrier-org/pyterrier), an open-source search engine.\n",
    "\n",
    "Building your own index can be already one way that you can try to improve upon this baseline (if you want to focus on creating good document representations). Other ways could include reformulating queries or tuning parameters or building better retrieval pipelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tira in /usr/local/lib/python3.10/dist-packages (0.0.134)\n",
      "Requirement already satisfied: ir-datasets in /usr/local/lib/python3.10/dist-packages (0.5.5)\n",
      "Requirement already satisfied: python-terrier in /usr/local/lib/python3.10/dist-packages (0.10.0)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from tira) (4.66.1)\n",
      "Requirement already satisfied: requests==2.*,>=2.26 in /usr/local/lib/python3.10/dist-packages (from tira) (2.31.0)\n",
      "Requirement already satisfied: numpy==1.* in /usr/local/lib/python3.10/dist-packages (from tira) (1.26.2)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tira) (23.2)\n",
      "Requirement already satisfied: docker==7.*,>=7.1.0 in /usr/local/lib/python3.10/dist-packages (from tira) (7.1.0)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from tira) (2.1.3)\n",
      "Requirement already satisfied: urllib3>=1.26.0 in /usr/local/lib/python3.10/dist-packages (from docker==7.*,>=7.1.0->tira) (2.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests==2.*,>=2.26->tira) (3.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests==2.*,>=2.26->tira) (3.3.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests==2.*,>=2.26->tira) (2023.11.17)\n",
      "Requirement already satisfied: lxml>=4.5.2 in /usr/local/lib/python3.10/dist-packages (from ir-datasets) (4.9.3)\n",
      "Requirement already satisfied: warc3-wet>=0.2.3 in /usr/local/lib/python3.10/dist-packages (from ir-datasets) (0.2.3)\n",
      "Requirement already satisfied: warc3-wet-clueweb09>=0.2.5 in /usr/local/lib/python3.10/dist-packages (from ir-datasets) (0.2.5)\n",
      "Requirement already satisfied: zlib-state>=0.1.3 in /usr/local/lib/python3.10/dist-packages (from ir-datasets) (0.1.6)\n",
      "Requirement already satisfied: unlzw3>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from ir-datasets) (0.2.2)\n",
      "Requirement already satisfied: beautifulsoup4>=4.4.1 in /usr/local/lib/python3.10/dist-packages (from ir-datasets) (4.12.2)\n",
      "Requirement already satisfied: trec-car-tools>=2.5.4 in /usr/local/lib/python3.10/dist-packages (from ir-datasets) (2.6)\n",
      "Requirement already satisfied: lz4>=3.1.10 in /usr/local/lib/python3.10/dist-packages (from ir-datasets) (4.3.2)\n",
      "Requirement already satisfied: inscriptis>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from ir-datasets) (2.3.2)\n",
      "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.10/dist-packages (from ir-datasets) (6.0.1)\n",
      "Requirement already satisfied: ijson>=3.1.3 in /usr/local/lib/python3.10/dist-packages (from ir-datasets) (3.2.3)\n",
      "Requirement already satisfied: pyautocorpus>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from ir-datasets) (0.1.12)\n",
      "Requirement already satisfied: nptyping==1.4.4 in /usr/local/lib/python3.10/dist-packages (from python-terrier) (1.4.4)\n",
      "Requirement already satisfied: pytrec-eval-terrier>=0.5.3 in /usr/local/lib/python3.10/dist-packages (from python-terrier) (0.5.6)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from python-terrier) (1.11.4)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from python-terrier) (1.3.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from python-terrier) (3.1.2)\n",
      "Requirement already satisfied: chest in /usr/local/lib/python3.10/dist-packages (from python-terrier) (0.2.3)\n",
      "Requirement already satisfied: deprecated in /usr/local/lib/python3.10/dist-packages (from python-terrier) (1.2.14)\n",
      "Requirement already satisfied: matchpy in /usr/local/lib/python3.10/dist-packages (from python-terrier) (0.5.5)\n",
      "Requirement already satisfied: statsmodels in /usr/local/lib/python3.10/dist-packages (from python-terrier) (0.14.0)\n",
      "Requirement already satisfied: wget in /usr/local/lib/python3.10/dist-packages (from python-terrier) (3.2)\n",
      "Requirement already satisfied: pyjnius>=1.4.2 in /usr/local/lib/python3.10/dist-packages (from python-terrier) (1.6.1)\n",
      "Requirement already satisfied: ir-measures>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from python-terrier) (0.3.3)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from python-terrier) (1.3.2)\n",
      "Requirement already satisfied: more-itertools in /usr/local/lib/python3.10/dist-packages (from python-terrier) (10.1.0)\n",
      "Requirement already satisfied: dill in /usr/local/lib/python3.10/dist-packages (from python-terrier) (0.3.7)\n",
      "Requirement already satisfied: typish>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from nptyping==1.4.4->python-terrier) (1.9.3)\n",
      "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4>=4.4.1->ir-datasets) (2.5)\n",
      "Requirement already satisfied: cwl-eval>=1.0.10 in /usr/local/lib/python3.10/dist-packages (from ir-measures>=0.3.1->python-terrier) (1.0.12)\n",
      "Requirement already satisfied: cbor>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from trec-car-tools>=2.5.4->ir-datasets) (1.0.0)\n",
      "Requirement already satisfied: heapdict in /usr/local/lib/python3.10/dist-packages (from chest->python-terrier) (1.0.1)\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.10/dist-packages (from deprecated->python-terrier) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->python-terrier) (2.1.3)\n",
      "Requirement already satisfied: multiset<3.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from matchpy->python-terrier) (2.1.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->tira) (2.8.2)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->tira) (2023.3)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->tira) (2023.3.post1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->python-terrier) (3.2.0)\n",
      "Requirement already satisfied: patsy>=0.5.2 in /usr/local/lib/python3.10/dist-packages (from statsmodels->python-terrier) (0.5.4)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from patsy>=0.5.2->statsmodels->python-terrier) (1.16.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# You only need to execute this cell if you are using Google Golab.\n",
    "# If you use GitHub Codespaces, everything is already installed.\n",
    "!pip3 install tira ir-datasets python-terrier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from tira.third_party_integrations import ensure_pyterrier_is_loaded, persist_and_normalize_run\n",
    "from tira.rest_api_client import Client\n",
    "import pyterrier as pt\n",
    "# stopword imports\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import spacy\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "# lemmatizer imports\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', 0)\n",
    "\n",
    "if not pt.started():\n",
    "    pt.init(boot_packages=['mam10eks:custom-terrier-token-processing:0.0.1'])\n",
    "    from jnius import autoclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.4.1\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.4.1/en_core_web_sm-3.4.1-py3-none-any.whl (12.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: spacy<3.5.0,>=3.4.0 in /usr/local/lib/python3.10/dist-packages (from en-core-web-sm==3.4.1) (3.4.0)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.10.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.4.0)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.0.9)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.26.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (23.2)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.4.8)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.0.8)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (8.1.12)\n",
      "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.4.2)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in /usr/local/lib/python3.10/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.0.12)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.1.2)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.0.10)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (4.66.1)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.0.10)\n",
      "Requirement already satisfied: setuptools in /usr/lib/python3/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (59.6.0)\n",
      "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.10/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.11.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.10.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.9.2)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.0.5)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.31.0)\n",
      "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.2.0)\n",
      "Requirement already satisfied: pathlib-abc==0.1.1 in /usr/local/lib/python3.10/dist-packages (from pathy>=0.3.5->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.1.1)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from pathy>=0.3.5->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (6.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<1.10.0,>=1.7.4->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (4.8.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.3.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2023.11.17)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.6)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.1.5)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.5.0,>=0.3.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (8.1.7)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.1.3)\n",
      "Requirement already satisfied: marisa-trie>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.2.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "# Create a REST client to the TIRA platform for retrieving the pre-indexed data.\n",
    "ensure_pyterrier_is_loaded()\n",
    "tira = Client()\n",
    "# spacy model\n",
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Stopword Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a quick look at the stopwords\n",
      "{'while', 'seeming', 'we', 'get', \"should've\", 've', 'itself', 'when', 'seemed', 'indeed', 'whether', 'third', 'is', 'part', '‘d', 'at', 'full', 'not', 'around', 'with', 'bill', 'y', 'two', 'anyway', 'through', 'onto', 'moreover', 'whole', 'done', 'serious', 'i', 'therefore', 'both', 'namely', 'five', 'been', 'please', 'various', 'thru', 'n‘t', 'via', 'somehow', 'too', 'ca', 'same', 'per', 'from', 'herself', 'many', 'down', 'mine', \"'d\", 'would', 'its', 'do', 'thereupon', 'meanwhile', 'go', 'having', 'ltd', 'become', 'whereafter', 'eg', 'used', 'during', 'give', 'a', 'under', 'wasn', 'hereafter', 'he', 'find', 'more', 'up', 'among', 'etc', \"wouldn't\", 'cant', '’m', 'about', 'afterwards', 'latterly', 'and', '’s', 'beyond', 'anyhow', 'should', \"wasn't\", 'something', \"'ve\", 'therein', \"she's\", 'con', 'ma', 'then', 'very', 'can', '’ve', 'less', \"needn't\", 'fill', 'much', 'as', 'four', 'enough', 'these', 'just', 'o', \"you'd\", 'some', 'by', 'beforehand', 'you', \"shan't\", 'whereas', 'but', 'aren', 'll', 'she', 'otherwise', 'sometime', 'if', 'inc', 'her', 'must', 'never', 'forty', 'only', \"you're\", 'made', 'amoungst', 'they', 'describe', 'whereby', 'almost', 'always', 'often', 'name', 'them', 'few', \"aren't\", 'nine', 'becoming', 'well', 't', 'myself', 'anywhere', \"mightn't\", 'who', 'put', 'isn', 'say', 'twenty', 'least', 'won', 'mill', 'found', 'could', 'am', 'another', 'whom', 'before', 'needn', \"'s\", \"isn't\", 'toward', 'ie', 'front', 'amongst', 'between', 'regarding', 'besides', 'will', 'thereby', 'our', 'whence', 'has', \"won't\", 'doing', 'un', 'whenever', '’ll', \"'m\", 'are', 'cannot', 'him', 'together', 'shouldn', 'bottom', 'was', \"mustn't\", 'couldnt', 'yourselves', 'hereupon', 'their', 'others', 'anyone', 'it', 'former', \"haven't\", 'fifteen', 'that', 'along', 'fifty', 'until', 'shan', 'system', 'his', \"'ll\", 'eleven', 'does', 'below', 'wouldn', 'whatever', 'see', '‘ve', 'did', 'next', 'thin', 'mightn', 'mostly', 'weren', 'even', 'twelve', 'except', 'the', 'last', 'whose', 'amount', 'quite', 'theirs', \"don't\", 'off', 'alone', 'each', 'somewhere', 'ever', 'else', 'still', \"weren't\", 'elsewhere', 'across', 'either', 'hereby', 'using', 'really', 'were', 'show', 'being', 'hasn', 'hadn', 'this', 'take', 're', 'in', 'seem', 'further', 'how', 'sincere', 'couldn', 'us', 'my', \"hasn't\", 'didn', 'whoever', 'throughout', 'keep', \"you've\", '‘re', \"doesn't\", 'doesn', 'd', '’re', 'thus', 'herein', 'wherein', \"it's\", 'first', 'sixty', 'however', 'mustn', 'due', '‘s', \"didn't\", 'have', 'without', 'also', 'over', 'everything', 'an', 'rather', 'other', 'm', 'no', 'haven', 'everyone', 'here', 'nowhere', 'behind', \"you'll\", 'noone', 'yourself', '‘m', 'your', 'hundred', 'all', 'though', 'someone', 'n’t', 'ain', 'thick', 'several', 'most', 'once', 'any', 'there', 'may', 'out', 'because', 'again', 'side', 'make', 'so', 'hasnt', 'to', 'nobody', 'wherever', 'unless', 'on', 'beside', 'ourselves', 'empty', 'be', 'whither', 'seems', 'becomes', 'those', 'eight', \"'re\", 'already', 'anything', 'back', \"that'll\", \"couldn't\", '‘ll', 'detail', 'of', 'cry', 'since', 'hers', 'himself', 'whereupon', 'everywhere', 'above', 'six', 'within', 'nevertheless', 'which', 'after', 'yours', 'sometimes', 'interest', \"n't\", 'where', 'own', \"hadn't\", 'into', 's', 'perhaps', 'although', 'me', 'thereafter', 'neither', 'fire', 'latter', 'top', 'against', 'upon', 'call', 'formerly', 'ours', 'three', 'move', 'might', 'such', 'don', 'thence', 'now', 'hence', 'towards', \"shouldn't\", 'yet', 'or', '’d', 'had', 'than', 'nothing', 'why', 'for', 'became', 'themselves', 'none', 'co', 'de', 'one', 'what', 'ten', 'every', 'nor'}\n"
     ]
    }
   ],
   "source": [
    "# download stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Generate custom stopword list\n",
    "nltk_stopwords = set(stopwords.words('english'))\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "spacy_stopwords = set(nlp.Defaults.stop_words)\n",
    "sklearn_stopwords = set(ENGLISH_STOP_WORDS)\n",
    "combined_stopwords = set.union(nltk_stopwords, spacy_stopwords, sklearn_stopwords)\n",
    "\n",
    "# output to verify stopwords\n",
    "print('a quick look at the stopwords')\n",
    "print(combined_stopwords)\n",
    "\n",
    "## Create and save stopword file\n",
    "file_path = \"../custom-stopwords/custom_stopwords.txt\"\n",
    "\n",
    "with open(file_path, 'w+') as file:\n",
    "    for element in combined_stopwords:\n",
    "        file.write(element + \"\\n\")\n",
    "\n",
    "# Set property for stopword file in PyTerrier\n",
    "pt.set_property('stopwords.filename', '../custom-stopwords/custom_stopwords.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Load the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Dataset...\n",
      "Dataset loaded.\n"
     ]
    }
   ],
   "source": [
    "print('Loading Dataset...')\n",
    "# This line creates an IRDSDataset object and registers it under the name provided as an argument.\n",
    "pt_dataset = pt.get_dataset('irds:ir-lab-sose-2024/ir-acl-anthology-20240504-training')\n",
    "print('Dataset loaded.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step x: Lemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic lemmatizer implementation\n",
    "# stanford lemmatizer\n",
    "def lemmatize(t):\n",
    "    lemmatizer = autoclass(\"org.terrier.terms.StanfordLemmatizer\")()\n",
    "    return lemmatizer.stem(t)\n",
    "\n",
    "# porterStemmer\n",
    "def stem(t):\n",
    "    stemmer = autoclass(\"org.terrier.terms.PorterStemmer\")()\n",
    "    return stemmer.stem(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Index Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Index...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ir-lab-sose-2024/ir-acl-anthology-20240504-training documents:   0%|          | 0/126958 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17:59:08.846 [ForkJoinPool-3-worker-3] WARN org.terrier.structures.indexing.Indexer - TermPipeline object org.terrier.terms.StanfordLemmatizer not found: java.lang.ClassNotFoundException: org.terrier.terms.StanfordLemmatizer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "java.lang.ClassNotFoundException: org.terrier.terms.StanfordLemmatizer\n",
      "\tat java.base/jdk.internal.loader.BuiltinClassLoader.loadClass(BuiltinClassLoader.java:581)\n",
      "\tat java.base/jdk.internal.loader.ClassLoaders$AppClassLoader.loadClass(ClassLoaders.java:178)\n",
      "\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)\n",
      "\tat java.base/java.lang.Class.forName0(Native Method)\n",
      "\tat java.base/java.lang.Class.forName(Class.java:398)\n",
      "\tat org.terrier.utility.ApplicationSetup.getClass(ApplicationSetup.java:416)\n",
      "\tat org.terrier.structures.indexing.Indexer.load_pipeline(Indexer.java:323)\n",
      "\tat org.terrier.structures.indexing.Indexer.init(Indexer.java:197)\n",
      "\tat org.terrier.structures.indexing.classical.BasicIndexer.<init>(BasicIndexer.java:183)\n",
      "\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n",
      "\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)\n",
      "\tat org.terrier.python.ParallelIndexer$3.apply(ParallelIndexer.java:126)\n",
      "\tat org.terrier.python.ParallelIndexer$3.apply(ParallelIndexer.java:120)\n",
      "\tat java.base/java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:195)\n",
      "\tat java.base/java.util.Spliterators$ArraySpliterator.forEachRemaining(Spliterators.java:948)\n",
      "\tat java.base/java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:484)\n",
      "\tat java.base/java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:474)\n",
      "\tat java.base/java.util.stream.ReduceOps$ReduceTask.doLeaf(ReduceOps.java:952)\n",
      "\tat java.base/java.util.stream.ReduceOps$ReduceTask.doLeaf(ReduceOps.java:926)\n",
      "\tat java.base/java.util.stream.AbstractTask.compute(AbstractTask.java:327)\n",
      "\tat java.base/java.util.concurrent.CountedCompleter.exec(CountedCompleter.java:746)\n",
      "\tat java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:290)\n",
      "\tat java.base/java.util.concurrent.ForkJoinTask.doInvoke(ForkJoinTask.java:408)\n",
      "\tat java.base/java.util.concurrent.ForkJoinTask.invoke(ForkJoinTask.java:736)\n",
      "\tat java.base/java.util.stream.ReduceOps$ReduceOp.evaluateParallel(ReduceOps.java:919)\n",
      "\tat java.base/java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:233)\n",
      "\tat java.base/java.util.stream.ReferencePipeline.reduce(ReferencePipeline.java:558)\n",
      "\tat org.terrier.python.ParallelIndexer$4.call(ParallelIndexer.java:140)\n",
      "\tat org.terrier.python.ParallelIndexer$4.call(ParallelIndexer.java:137)\n",
      "\tat java.base/java.util.concurrent.ForkJoinTask$AdaptedCallable.exec(ForkJoinTask.java:1448)\n",
      "\tat java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:290)\n",
      "\tat java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1020)\n",
      "\tat java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1656)\n",
      "\tat java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1594)\n",
      "\tat java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:183)\n",
      "ir-lab-sose-2024/ir-acl-anthology-20240504-training documents: 100%|██████████| 126958/126958 [00:11<00:00, 10672.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index created.\n"
     ]
    }
   ],
   "source": [
    "print('Building Index...')\n",
    "\n",
    "\n",
    "\n",
    "def create_index(pt_dataset, stopwords):\n",
    "    # added lemmatizer/stemmer\n",
    "    indexer = pt.IterDictIndexer(\"/tmp/index\", overwrite=True, meta={'docno': 100, 'text': 20480}, stopwords=stopwords, stemmer='StanfordLemmatizer')\n",
    "    index_ref = indexer.index(pt_dataset)\n",
    "    return pt.IndexFactory.of(index_ref)\n",
    "\n",
    "\n",
    "index = create_index(pt_dataset.get_corpus_iter(), combined_stopwords)\n",
    "print('Index created.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Define the Retrieval Pipeline + Query Expansion\n",
    "\n",
    "We will define a BM25 retrieval pipeline as baseline. For details, see:\n",
    "\n",
    "- [https://pyterrier.readthedocs.io](https://pyterrier.readthedocs.io)\n",
    "- [https://github.com/terrier-org/ecir2021tutorial](https://github.com/terrier-org/ecir2021tutorial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17:59:28.190 [main] ERROR org.terrier.terms.BaseTermPipelineAccessor - TermPipeline object org.terrier.terms.StanfordLemmatizer not found\n",
      "java.lang.ClassNotFoundException: org.terrier.terms.StanfordLemmatizer\n",
      "\tat java.base/jdk.internal.loader.BuiltinClassLoader.loadClass(BuiltinClassLoader.java:581)\n",
      "\tat java.base/jdk.internal.loader.ClassLoaders$AppClassLoader.loadClass(ClassLoaders.java:178)\n",
      "\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)\n",
      "\tat java.base/java.lang.Class.forName0(Native Method)\n",
      "\tat java.base/java.lang.Class.forName(Class.java:398)\n",
      "\tat org.terrier.utility.ApplicationSetup.getClass(ApplicationSetup.java:421)\n",
      "\tat org.terrier.terms.BaseTermPipelineAccessor.<init>(BaseTermPipelineAccessor.java:70)\n",
      "\tat org.terrier.querying.ApplyTermPipeline.load_pipeline(ApplyTermPipeline.java:90)\n",
      "\tat org.terrier.querying.ApplyTermPipeline.<init>(ApplyTermPipeline.java:80)\n",
      "\tat org.terrier.querying.ApplyTermPipeline.<init>(ApplyTermPipeline.java:74)\n"
     ]
    }
   ],
   "source": [
    "bo1 = pt.rewrite.Bo1QueryExpansion(index)\n",
    "\n",
    "# definition of BM25 pipeline with stopword index\n",
    "bm25 = pt.BatchRetrieve(index, wmodel=\"BM25\")\n",
    "\n",
    "bm25_expand = bm25 >> bo1 >> bm25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Create the Run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First, we have a short look at the first three topics:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qid</th>\n",
       "      <th>query</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>retrieval system improving effectiveness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>machine learning language identification</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>social media detect self harm</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  qid                                     query\n",
       "0  1   retrieval system improving effectiveness\n",
       "1  2   machine learning language identification\n",
       "2  3   social media detect self harm           "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('First, we have a short look at the first three topics:')\n",
    "\n",
    "pt_dataset.get_topics('text').head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create run\n",
      "17:59:31.866 [main] ERROR org.terrier.terms.BaseTermPipelineAccessor - TermPipeline object org.terrier.terms.StanfordLemmatizer not found\n",
      "java.lang.ClassNotFoundException: org.terrier.terms.StanfordLemmatizer\n",
      "\tat java.base/jdk.internal.loader.BuiltinClassLoader.loadClass(BuiltinClassLoader.java:581)\n",
      "\tat java.base/jdk.internal.loader.ClassLoaders$AppClassLoader.loadClass(ClassLoaders.java:178)\n",
      "\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)\n",
      "\tat java.base/java.lang.Class.forName0(Native Method)\n",
      "\tat java.base/java.lang.Class.forName(Class.java:398)\n",
      "\tat org.terrier.utility.ApplicationSetup.getClass(ApplicationSetup.java:421)\n",
      "\tat org.terrier.terms.BaseTermPipelineAccessor.<init>(BaseTermPipelineAccessor.java:70)\n",
      "\tat org.terrier.querying.ApplyTermPipeline.load_pipeline(ApplyTermPipeline.java:90)\n",
      "\tat org.terrier.querying.ApplyTermPipeline.getPipeline(ApplyTermPipeline.java:150)\n",
      "\tat org.terrier.querying.ApplyTermPipeline.process(ApplyTermPipeline.java:162)\n",
      "\tat org.terrier.querying.LocalManager.runSearchRequest(LocalManager.java:895)\n",
      "Done. Here are the first 10 entries of the run\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qid</th>\n",
       "      <th>docid</th>\n",
       "      <th>docno</th>\n",
       "      <th>rank</th>\n",
       "      <th>score</th>\n",
       "      <th>query_0</th>\n",
       "      <th>query</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>91391</td>\n",
       "      <td>2004.ecir_conference-2004.21</td>\n",
       "      <td>0</td>\n",
       "      <td>19.120880</td>\n",
       "      <td>retrieval system improving effectiveness</td>\n",
       "      <td>applypipeline:off retrieval^1.248593174 system^1.049840886 improving^1.173257422 effectiveness^1.301538472 by^0.075992356 the^0.062643036 of^0.041941845 multiple^0.000000000 discussed^0.000000000 use^0.000000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>123051</td>\n",
       "      <td>2002.ipm_journal-ir0volumeA38A1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>19.034918</td>\n",
       "      <td>retrieval system improving effectiveness</td>\n",
       "      <td>applypipeline:off retrieval^1.248593174 system^1.049840886 improving^1.173257422 effectiveness^1.301538472 by^0.075992356 the^0.062643036 of^0.041941845 multiple^0.000000000 discussed^0.000000000 use^0.000000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>90655</td>\n",
       "      <td>2017.airs_conference-2017.1</td>\n",
       "      <td>2</td>\n",
       "      <td>19.026703</td>\n",
       "      <td>retrieval system improving effectiveness</td>\n",
       "      <td>applypipeline:off retrieval^1.248593174 system^1.049840886 improving^1.173257422 effectiveness^1.301538472 by^0.075992356 the^0.062643036 of^0.041941845 multiple^0.000000000 discussed^0.000000000 use^0.000000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>125137</td>\n",
       "      <td>1989.ipm_journal-ir0volumeA25A4.2</td>\n",
       "      <td>3</td>\n",
       "      <td>19.013045</td>\n",
       "      <td>retrieval system improving effectiveness</td>\n",
       "      <td>applypipeline:off retrieval^1.248593174 system^1.049840886 improving^1.173257422 effectiveness^1.301538472 by^0.075992356 the^0.062643036 of^0.041941845 multiple^0.000000000 discussed^0.000000000 use^0.000000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>53327</td>\n",
       "      <td>O07-2010</td>\n",
       "      <td>4</td>\n",
       "      <td>18.890801</td>\n",
       "      <td>retrieval system improving effectiveness</td>\n",
       "      <td>applypipeline:off retrieval^1.248593174 system^1.049840886 improving^1.173257422 effectiveness^1.301538472 by^0.075992356 the^0.062643036 of^0.041941845 multiple^0.000000000 discussed^0.000000000 use^0.000000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>84876</td>\n",
       "      <td>2016.ntcir_conference-2016.90</td>\n",
       "      <td>5</td>\n",
       "      <td>18.700888</td>\n",
       "      <td>retrieval system improving effectiveness</td>\n",
       "      <td>applypipeline:off retrieval^1.248593174 system^1.049840886 improving^1.173257422 effectiveness^1.301538472 by^0.075992356 the^0.062643036 of^0.041941845 multiple^0.000000000 discussed^0.000000000 use^0.000000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>78846</td>\n",
       "      <td>2016.iir_workshop-2016.13</td>\n",
       "      <td>6</td>\n",
       "      <td>18.355955</td>\n",
       "      <td>retrieval system improving effectiveness</td>\n",
       "      <td>applypipeline:off retrieval^1.248593174 system^1.049840886 improving^1.173257422 effectiveness^1.301538472 by^0.075992356 the^0.062643036 of^0.041941845 multiple^0.000000000 discussed^0.000000000 use^0.000000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>125817</td>\n",
       "      <td>2005.ipm_journal-ir0volumeA41A5.11</td>\n",
       "      <td>7</td>\n",
       "      <td>17.709968</td>\n",
       "      <td>retrieval system improving effectiveness</td>\n",
       "      <td>applypipeline:off retrieval^1.248593174 system^1.049840886 improving^1.173257422 effectiveness^1.301538472 by^0.075992356 the^0.062643036 of^0.041941845 multiple^0.000000000 discussed^0.000000000 use^0.000000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>126826</td>\n",
       "      <td>2007.tois_journal-ir0volumeA26A1.4</td>\n",
       "      <td>8</td>\n",
       "      <td>16.507355</td>\n",
       "      <td>retrieval system improving effectiveness</td>\n",
       "      <td>applypipeline:off retrieval^1.248593174 system^1.049840886 improving^1.173257422 effectiveness^1.301538472 by^0.075992356 the^0.062643036 of^0.041941845 multiple^0.000000000 discussed^0.000000000 use^0.000000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>121640</td>\n",
       "      <td>2005.sigirjournals_journal-ir0volumeA39A2.12</td>\n",
       "      <td>9</td>\n",
       "      <td>16.231794</td>\n",
       "      <td>retrieval system improving effectiveness</td>\n",
       "      <td>applypipeline:off retrieval^1.248593174 system^1.049840886 improving^1.173257422 effectiveness^1.301538472 by^0.075992356 the^0.062643036 of^0.041941845 multiple^0.000000000 discussed^0.000000000 use^0.000000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  qid   docid                                         docno  rank      score  \\\n",
       "0  1   91391   2004.ecir_conference-2004.21                  0     19.120880   \n",
       "1  1   123051  2002.ipm_journal-ir0volumeA38A1.0             1     19.034918   \n",
       "2  1   90655   2017.airs_conference-2017.1                   2     19.026703   \n",
       "3  1   125137  1989.ipm_journal-ir0volumeA25A4.2             3     19.013045   \n",
       "4  1   53327   O07-2010                                      4     18.890801   \n",
       "5  1   84876   2016.ntcir_conference-2016.90                 5     18.700888   \n",
       "6  1   78846   2016.iir_workshop-2016.13                     6     18.355955   \n",
       "7  1   125817  2005.ipm_journal-ir0volumeA41A5.11            7     17.709968   \n",
       "8  1   126826  2007.tois_journal-ir0volumeA26A1.4            8     16.507355   \n",
       "9  1   121640  2005.sigirjournals_journal-ir0volumeA39A2.12  9     16.231794   \n",
       "\n",
       "                                    query_0  \\\n",
       "0  retrieval system improving effectiveness   \n",
       "1  retrieval system improving effectiveness   \n",
       "2  retrieval system improving effectiveness   \n",
       "3  retrieval system improving effectiveness   \n",
       "4  retrieval system improving effectiveness   \n",
       "5  retrieval system improving effectiveness   \n",
       "6  retrieval system improving effectiveness   \n",
       "7  retrieval system improving effectiveness   \n",
       "8  retrieval system improving effectiveness   \n",
       "9  retrieval system improving effectiveness   \n",
       "\n",
       "                                                                                                                                                                                                                 query  \n",
       "0  applypipeline:off retrieval^1.248593174 system^1.049840886 improving^1.173257422 effectiveness^1.301538472 by^0.075992356 the^0.062643036 of^0.041941845 multiple^0.000000000 discussed^0.000000000 use^0.000000000  \n",
       "1  applypipeline:off retrieval^1.248593174 system^1.049840886 improving^1.173257422 effectiveness^1.301538472 by^0.075992356 the^0.062643036 of^0.041941845 multiple^0.000000000 discussed^0.000000000 use^0.000000000  \n",
       "2  applypipeline:off retrieval^1.248593174 system^1.049840886 improving^1.173257422 effectiveness^1.301538472 by^0.075992356 the^0.062643036 of^0.041941845 multiple^0.000000000 discussed^0.000000000 use^0.000000000  \n",
       "3  applypipeline:off retrieval^1.248593174 system^1.049840886 improving^1.173257422 effectiveness^1.301538472 by^0.075992356 the^0.062643036 of^0.041941845 multiple^0.000000000 discussed^0.000000000 use^0.000000000  \n",
       "4  applypipeline:off retrieval^1.248593174 system^1.049840886 improving^1.173257422 effectiveness^1.301538472 by^0.075992356 the^0.062643036 of^0.041941845 multiple^0.000000000 discussed^0.000000000 use^0.000000000  \n",
       "5  applypipeline:off retrieval^1.248593174 system^1.049840886 improving^1.173257422 effectiveness^1.301538472 by^0.075992356 the^0.062643036 of^0.041941845 multiple^0.000000000 discussed^0.000000000 use^0.000000000  \n",
       "6  applypipeline:off retrieval^1.248593174 system^1.049840886 improving^1.173257422 effectiveness^1.301538472 by^0.075992356 the^0.062643036 of^0.041941845 multiple^0.000000000 discussed^0.000000000 use^0.000000000  \n",
       "7  applypipeline:off retrieval^1.248593174 system^1.049840886 improving^1.173257422 effectiveness^1.301538472 by^0.075992356 the^0.062643036 of^0.041941845 multiple^0.000000000 discussed^0.000000000 use^0.000000000  \n",
       "8  applypipeline:off retrieval^1.248593174 system^1.049840886 improving^1.173257422 effectiveness^1.301538472 by^0.075992356 the^0.062643036 of^0.041941845 multiple^0.000000000 discussed^0.000000000 use^0.000000000  \n",
       "9  applypipeline:off retrieval^1.248593174 system^1.049840886 improving^1.173257422 effectiveness^1.301538472 by^0.075992356 the^0.062643036 of^0.041941845 multiple^0.000000000 discussed^0.000000000 use^0.000000000  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Create run')\n",
    "run = bm25_expand(pt_dataset.get_topics('text'))\n",
    "print('Done. Here are the first 10 entries of the run')\n",
    "run.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7: Persist the run file for subsequent evaluations\n",
    "\n",
    "The output of a prototypical retrieval system is a run file. This run file can later (optimally in a different notebook) be statistically evaluated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done. run file is stored under \"../runs/run.txt\".\n"
     ]
    }
   ],
   "source": [
    "persist_and_normalize_run(run, system_name='bm25-stopwords-query-expansion', output_file='../runs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### custom eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are multiple query fields available: ('text', 'title', 'query', 'description', 'narrative'). To use with pyterrier, provide variant or modify dataframe to add query column.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>ndcg_cut.10</th>\n",
       "      <th>recip_rank</th>\n",
       "      <th>recall_100</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BM25 (Own)</td>\n",
       "      <td>0.274717</td>\n",
       "      <td>0.463803</td>\n",
       "      <td>0.459857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BM 25 (Baseline)</td>\n",
       "      <td>0.374041</td>\n",
       "      <td>0.579877</td>\n",
       "      <td>0.601333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sparse Cross Encoder</td>\n",
       "      <td>0.366460</td>\n",
       "      <td>0.612980</td>\n",
       "      <td>0.601333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RankZephyr</td>\n",
       "      <td>0.347070</td>\n",
       "      <td>0.568413</td>\n",
       "      <td>0.601333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   name  ndcg_cut.10  recip_rank  recall_100\n",
       "0  BM25 (Own)            0.274717     0.463803    0.459857  \n",
       "1  BM 25 (Baseline)      0.374041     0.579877    0.601333  \n",
       "2  Sparse Cross Encoder  0.366460     0.612980    0.601333  \n",
       "3  RankZephyr            0.347070     0.568413    0.601333  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bm25 = pt.io.read_results('../runs/run.txt')\n",
    "\n",
    "bm25_baseline = tira.pt.from_submission('ir-benchmarks/tira-ir-starter/BM25 (tira-ir-starter-pyterrier)', pt_dataset)\n",
    "sparse_cross_encoder = tira.pt.from_submission('ir-benchmarks/fschlatt/sparse-cross-encoder-4-512', pt_dataset)\n",
    "rank_zephyr = tira.pt.from_submission('workshop-on-open-web-search/fschlatt/rank-zephyr', pt_dataset)\n",
    "\n",
    "pt.Experiment(\n",
    "    [bm25, bm25_baseline, sparse_cross_encoder, rank_zephyr],\n",
    "    pt_dataset.get_topics(),\n",
    "    pt_dataset.get_qrels(),\n",
    "    [\"ndcg_cut.10\", \"recip_rank\", \"recall_100\"],\n",
    "    names=[\"BM25 (Own)\", \"BM 25 (Baseline)\", \"Sparse Cross Encoder\", \"RankZephyr\"]\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
