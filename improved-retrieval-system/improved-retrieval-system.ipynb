{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IR Lab SoSe 2024: Baseline Retrieval System\n",
    "\n",
    "This jupyter notebook serves as baseline retrieval system that you can try to improve upon.\n",
    "We will use the a corpus of scientific papers (title + abstracts) from the fields of information retrieval and natural language processing (the [IR Anthology](https://ir.webis.de/anthology/) and the [ACL Anthology](https://aclanthology.org/)). This serves Jupyter notebook only serves as retrieval system, i.e., it gets a set of information needs (topics) and a corpus as input and produces a run file as output. Please do evaluations in a new dedicated notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Import Libraries\n",
    "\n",
    "We will use [tira](https://www.tira.io/), an information retrieval shared task platform, for loading the (pre-built) retrieval index and [ir_dataset](https://ir-datasets.com/) to subsequently build a retrieval system with [PyTerrier](https://github.com/terrier-org/pyterrier), an open-source search engine.\n",
    "\n",
    "Building your own index can be already one way that you can try to improve upon this baseline (if you want to focus on creating good document representations). Other ways could include reformulating queries or tuning parameters or building better retrieval pipelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tira in /usr/local/lib/python3.10/dist-packages (0.0.134)\n",
      "Requirement already satisfied: ir-datasets in /usr/local/lib/python3.10/dist-packages (0.5.5)\n",
      "Requirement already satisfied: python-terrier in /usr/local/lib/python3.10/dist-packages (0.10.0)\n",
      "Requirement already satisfied: requests==2.*,>=2.26 in /usr/local/lib/python3.10/dist-packages (from tira) (2.31.0)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from tira) (4.66.1)\n",
      "Requirement already satisfied: numpy==1.* in /usr/local/lib/python3.10/dist-packages (from tira) (1.26.2)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tira) (23.2)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from tira) (2.1.3)\n",
      "Requirement already satisfied: docker==7.*,>=7.1.0 in /usr/local/lib/python3.10/dist-packages (from tira) (7.1.0)\n",
      "Requirement already satisfied: urllib3>=1.26.0 in /usr/local/lib/python3.10/dist-packages (from docker==7.*,>=7.1.0->tira) (2.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests==2.*,>=2.26->tira) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests==2.*,>=2.26->tira) (3.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests==2.*,>=2.26->tira) (2023.11.17)\n",
      "Requirement already satisfied: unlzw3>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from ir-datasets) (0.2.2)\n",
      "Requirement already satisfied: beautifulsoup4>=4.4.1 in /usr/local/lib/python3.10/dist-packages (from ir-datasets) (4.12.2)\n",
      "Requirement already satisfied: lxml>=4.5.2 in /usr/local/lib/python3.10/dist-packages (from ir-datasets) (4.9.3)\n",
      "Requirement already satisfied: trec-car-tools>=2.5.4 in /usr/local/lib/python3.10/dist-packages (from ir-datasets) (2.6)\n",
      "Requirement already satisfied: lz4>=3.1.10 in /usr/local/lib/python3.10/dist-packages (from ir-datasets) (4.3.2)\n",
      "Requirement already satisfied: warc3-wet>=0.2.3 in /usr/local/lib/python3.10/dist-packages (from ir-datasets) (0.2.3)\n",
      "Requirement already satisfied: zlib-state>=0.1.3 in /usr/local/lib/python3.10/dist-packages (from ir-datasets) (0.1.6)\n",
      "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.10/dist-packages (from ir-datasets) (6.0.1)\n",
      "Requirement already satisfied: pyautocorpus>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from ir-datasets) (0.1.12)\n",
      "Requirement already satisfied: ijson>=3.1.3 in /usr/local/lib/python3.10/dist-packages (from ir-datasets) (3.2.3)\n",
      "Requirement already satisfied: inscriptis>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from ir-datasets) (2.3.2)\n",
      "Requirement already satisfied: warc3-wet-clueweb09>=0.2.5 in /usr/local/lib/python3.10/dist-packages (from ir-datasets) (0.2.5)\n",
      "Requirement already satisfied: deprecated in /usr/local/lib/python3.10/dist-packages (from python-terrier) (1.2.14)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from python-terrier) (1.11.4)\n",
      "Requirement already satisfied: pyjnius>=1.4.2 in /usr/local/lib/python3.10/dist-packages (from python-terrier) (1.6.1)\n",
      "Requirement already satisfied: pytrec-eval-terrier>=0.5.3 in /usr/local/lib/python3.10/dist-packages (from python-terrier) (0.5.6)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from python-terrier) (1.3.2)\n",
      "Requirement already satisfied: nptyping==1.4.4 in /usr/local/lib/python3.10/dist-packages (from python-terrier) (1.4.4)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from python-terrier) (1.3.2)\n",
      "Requirement already satisfied: ir-measures>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from python-terrier) (0.3.3)\n",
      "Requirement already satisfied: more-itertools in /usr/local/lib/python3.10/dist-packages (from python-terrier) (10.1.0)\n",
      "Requirement already satisfied: wget in /usr/local/lib/python3.10/dist-packages (from python-terrier) (3.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from python-terrier) (3.1.2)\n",
      "Requirement already satisfied: matchpy in /usr/local/lib/python3.10/dist-packages (from python-terrier) (0.5.5)\n",
      "Requirement already satisfied: chest in /usr/local/lib/python3.10/dist-packages (from python-terrier) (0.2.3)\n",
      "Requirement already satisfied: statsmodels in /usr/local/lib/python3.10/dist-packages (from python-terrier) (0.14.0)\n",
      "Requirement already satisfied: dill in /usr/local/lib/python3.10/dist-packages (from python-terrier) (0.3.7)\n",
      "Requirement already satisfied: typish>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from nptyping==1.4.4->python-terrier) (1.9.3)\n",
      "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4>=4.4.1->ir-datasets) (2.5)\n",
      "Requirement already satisfied: cwl-eval>=1.0.10 in /usr/local/lib/python3.10/dist-packages (from ir-measures>=0.3.1->python-terrier) (1.0.12)\n",
      "Requirement already satisfied: cbor>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from trec-car-tools>=2.5.4->ir-datasets) (1.0.0)\n",
      "Requirement already satisfied: heapdict in /usr/local/lib/python3.10/dist-packages (from chest->python-terrier) (1.0.1)\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.10/dist-packages (from deprecated->python-terrier) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->python-terrier) (2.1.3)\n",
      "Requirement already satisfied: multiset<3.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from matchpy->python-terrier) (2.1.1)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->tira) (2023.3.post1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->tira) (2.8.2)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->tira) (2023.3)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->python-terrier) (3.2.0)\n",
      "Requirement already satisfied: patsy>=0.5.2 in /usr/local/lib/python3.10/dist-packages (from statsmodels->python-terrier) (0.5.4)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from patsy>=0.5.2->statsmodels->python-terrier) (1.16.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# You only need to execute this cell if you are using Google Golab.\n",
    "# If you use GitHub Codespaces, everything is already installed.\n",
    "!pip3 install tira ir-datasets python-terrier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTerrier 0.10.0 has loaded Terrier 5.8 (built by craigm on 2023-11-01 18:05) and terrier-helper 0.0.8\n",
      "\n",
      "No etc/terrier.properties, using terrier.default.properties for bootstrap configuration.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.4.1\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.4.1/en_core_web_sm-3.4.1-py3-none-any.whl (12.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hRequirement already satisfied: spacy<3.5.0,>=3.4.0 in /usr/local/lib/python3.10/dist-packages (from en-core-web-sm==3.4.1) (3.4.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.10.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.9.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (23.2)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.0.10)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.0.10)\n",
      "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.4.2)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.26.2)\n",
      "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.10/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.11.0)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.1.2)\n",
      "Requirement already satisfied: setuptools in /usr/lib/python3/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (59.6.0)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.0.5)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.4.8)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.4.0)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.0.9)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.10.1)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (8.1.12)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.0.8)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in /usr/local/lib/python3.10/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.0.12)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (4.66.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.31.0)\n",
      "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.2.0)\n",
      "Requirement already satisfied: pathlib-abc==0.1.1 in /usr/local/lib/python3.10/dist-packages (from pathy>=0.3.5->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.1.1)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from pathy>=0.3.5->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (6.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<1.10.0,>=1.7.4->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (4.8.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2023.11.17)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.6)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.1.5)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.7.11)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.5.0,>=0.3.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (8.1.7)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.1.3)\n",
      "Requirement already satisfied: marisa-trie>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.2.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\nif not pt.started():\\n    pt.init(boot_packages=['mam10eks:custom-terrier-token-processing:0.0.1'])\\n    from jnius import autoclass\\n\""
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Imports\n",
    "from tira.third_party_integrations import ensure_pyterrier_is_loaded, persist_and_normalize_run\n",
    "from tira.rest_api_client import Client\n",
    "import pyterrier as pt\n",
    "# stopword imports\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import spacy\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "# lemmatizer imports\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', 0)\n",
    "# sklearn import\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Create a REST client to the TIRA platform for retrieving the pre-indexed data.\n",
    "ensure_pyterrier_is_loaded()\n",
    "tira = Client()\n",
    "# spacy model\n",
    "!python -m spacy download en_core_web_sm\n",
    "\n",
    "from jnius import autoclass\n",
    "'''\n",
    "if not pt.started():\n",
    "    pt.init(boot_packages=['mam10eks:custom-terrier-token-processing:0.0.1'])\n",
    "    from jnius import autoclass\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Stopword Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a quick look at the stopwords\n",
      "{'whereby', 'together', 'hasnt', 'five', \"shouldn't\", 'whole', 'just', 'off', 'sincere', 'my', 'side', 'this', 'whenever', 'ours', 'put', 'yourself', 'mustn', 'have', 'go', 'whereupon', 'down', '’re', 'seems', 'amount', 'ltd', 'through', 'sometime', 'via', \"mightn't\", 'ever', 'aren', 'became', 'too', 'was', 'becoming', 'having', 'until', 'part', \"wouldn't\", 'someone', 'already', '‘ve', 'm', 'doing', 't', 'it', 'detail', 'doesn', 'within', 'quite', 'latterly', 'nothing', 'another', 'hasn', 'afterwards', 'hers', 'am', 'less', 'take', 'mill', 'two', 'shouldn', 'alone', 'everyone', 'made', 'what', 'over', \"didn't\", 'say', 'therefore', '’s', '‘s', \"aren't\", 'mostly', 'wherein', 'thru', 'perhaps', 'eleven', 'con', 'you', \"needn't\", 'been', 'which', 'll', 'while', 'under', 'one', \"won't\", 'mightn', 'ca', 've', 'she', 'd', 'ma', \"hasn't\", 'latter', 'anywhere', \"should've\", 'why', 'will', 'hundred', 'noone', 'hence', 'towards', 'either', 'sixty', 'front', 'weren', 'as', 'herself', 'around', 'y', 'these', 'empty', 'whereafter', 'six', 'myself', 'toward', \"'s\", 'itself', \"you're\", 'except', 'becomes', 'etc', 'those', 'again', 'namely', 'thence', 'often', \"'ve\", \"you'd\", 'your', 'hereby', 'several', 'others', 'any', 'neither', 'whose', 'call', '‘m', 'much', 'only', \"that'll\", 'of', 'anything', 'co', 'thick', 'his', 'by', 'some', 'cry', 'when', 'would', 'system', 'before', 'how', 'wasn', 'haven', 'has', 'enough', 'full', '’ll', 'whence', 'get', 'is', 'serious', 'behind', 'regarding', 'us', 'fill', \"haven't\", 'so', 'however', 'inc', 'and', \"couldn't\", 'me', 'meanwhile', 'even', 'thereupon', 'do', 'four', 'top', 'whom', 'might', 'formerly', 'see', 'we', 'n‘t', \"doesn't\", 'give', 'against', 'after', 'should', 'our', 'all', 'up', 'could', \"hadn't\", 'ie', 'at', 'but', 'almost', 'may', 'hadn', 'very', 'indeed', 'in', 'make', 'he', 'couldn', 'does', 'besides', 'or', 'moreover', 'fifteen', 'interest', 'not', 'without', 'then', 'same', 'upon', 'name', 'had', 'couldnt', 'back', 'shan', 'own', 'twelve', 'nevertheless', 'keep', 'where', 'hereupon', 'don', 'though', 'there', 'unless', 'find', 'here', 'really', \"weren't\", \"don't\", 'nobody', 'beyond', 'themselves', 'a', 'rather', 'now', 'their', 'describe', 'across', 'among', 'were', 'forty', \"n't\", 'mine', 'herein', 'thus', 'n’t', 'ain', 'fire', 'sometimes', 'isn', 'bill', 'done', 'un', 'into', 'beforehand', 'who', 'least', 'nor', 'also', 'many', \"'m\", 'i', 'somewhere', \"wasn't\", 'about', 'anyone', 'everywhere', 'due', 'de', 'won', 'cant', 'first', 'nowhere', 'theirs', 'next', 're', 'other', 'yet', 'eight', 'such', 'from', 'using', 'beside', 'didn', 'always', 'her', 'otherwise', \"she's\", 'whatever', 'thereby', 'third', 'therein', 'an', \"'re\", 'somehow', 'wherever', 'throughout', 'move', 'seemed', 's', 'anyway', 'him', 'yours', 'to', 'few', 'below', 'everything', 'be', 'show', 'else', 'although', '‘d', 'cannot', 'no', 'out', 'did', 'can', \"'d\", \"it's\", 'every', 'thereafter', 'onto', 'used', 'eg', 'more', 'them', \"'ll\", 'fifty', 'must', 'they', 'amongst', \"mustn't\", 'three', 'amoungst', 'former', '’m', 'hereafter', 'seem', 'along', 'found', 'during', 'none', '’ve', 'wouldn', 'are', 'ten', 'whither', \"you'll\", 'seeming', 'twenty', 'various', 'nine', 'become', 'whether', 'whereas', 'with', 'between', 'something', 'its', 'both', 'because', 'o', 'anyhow', '‘ll', 'the', 'that', 'well', 'himself', 'bottom', 'on', 'still', \"isn't\", \"you've\", 'whoever', \"shan't\", 'if', 'last', 'most', 'for', 'since', 'needn', 'never', '‘re', 'ourselves', 'please', '’d', 'further', 'each', 'elsewhere', 'yourselves', 'being', 'once', 'above', 'than', 'thin', 'per'}\n"
     ]
    }
   ],
   "source": [
    "# download stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Generate custom stopword list\n",
    "nltk_stopwords = set(stopwords.words('english'))\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "spacy_stopwords = set(nlp.Defaults.stop_words)\n",
    "sklearn_stopwords = set(ENGLISH_STOP_WORDS)\n",
    "combined_stopwords = set.union(nltk_stopwords, spacy_stopwords, sklearn_stopwords)\n",
    "\n",
    "# output to verify stopwords\n",
    "print('a quick look at the stopwords')\n",
    "print(combined_stopwords)\n",
    "\n",
    "## Create and save stopword file\n",
    "# file_path = \"../custom-stopwords/custom_stopwords.txt\"\n",
    "file_path = \"./custom_stopwords.txt\"\n",
    "\n",
    "with open(file_path, 'w+') as file:\n",
    "    for element in combined_stopwords:\n",
    "        file.write(element + \"\\n\")\n",
    "\n",
    "# Set property for stopword file in PyTerrier\n",
    "# pt.set_property('stopwords.filename', '../custom-stopwords/custom_stopwords.txt')\n",
    "pt.set_property('stopwords.filename', './custom_stopwords.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Load the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Dataset...\n",
      "Dataset loaded.\n"
     ]
    }
   ],
   "source": [
    "print('Loading Dataset...')\n",
    "# This line creates an IRDSDataset object and registers it under the name provided as an argument.\n",
    "pt_dataset = pt.get_dataset('irds:ir-lab-sose-2024/ir-acl-anthology-20240504-training')\n",
    "print('Dataset loaded.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step x: Lemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic lemmatizer implementation\n",
    "def lemmatize(t):\n",
    "    lemmatizer = autoclass(\"org.terrier.terms.StanfordLemmatizer\")()\n",
    "    return lemmatizer.stem(t)\n",
    "\n",
    "# porterStemmer\n",
    "def stem(t):\n",
    "    stemmer = autoclass(\"org.terrier.terms.PorterStemmer\")()\n",
    "    return stemmer.stem(t)\n",
    "\n",
    "# lemurKrovetzStemmer\n",
    "def stem_krovetz(t):\n",
    "    stemmer = autoclass(\"org.terrier.terms.LemurKrovetzStemmer\")()\n",
    "    return stemmer.stem(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Index Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Index...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ir-lab-sose-2024/ir-acl-anthology-20240504-training documents: 100%|██████████| 126958/126958 [00:13<00:00, 9713.06it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index created.\n"
     ]
    }
   ],
   "source": [
    "print('Building Index...')\n",
    "\n",
    "def create_index(pt_dataset, stopwords):\n",
    "    # added lemmatizer\n",
    "    indexer = pt.IterDictIndexer(\"/tmp/index\", overwrite=True, meta={'docno': 100, 'text': 20480}, stopwords=stopwords, stemmer='PorterStemmer') # stemmer needs to be set\n",
    "    index_ref = indexer.index(pt_dataset)\n",
    "    return pt.IndexFactory.of(index_ref)\n",
    "\n",
    "# custom stopword index\n",
    "index = create_index(pt_dataset.get_corpus_iter(), combined_stopwords)\n",
    "\n",
    "# A (pre-built) PyTerrier index loaded from TIRA\n",
    "# index = tira.pt.index('ir-lab-sose-2024/tira-ir-starter/Index (tira-ir-starter-pyterrier)', pt_dataset)\n",
    "print('Index created.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Define the Retrieval Pipeline\n",
    "\n",
    "We will define a BM25 retrieval pipeline as baseline. For details, see:\n",
    "\n",
    "- [https://pyterrier.readthedocs.io](https://pyterrier.readthedocs.io)\n",
    "- [https://github.com/terrier-org/ecir2021tutorial](https://github.com/terrier-org/ecir2021tutorial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# definition of pipelines\n",
    "bm25 = pt.BatchRetrieve(index, wmodel=\"BM25\")\n",
    "tf = pt.BatchRetrieve(index, wmodel=\"Tf\")\n",
    "pl2 = pt.BatchRetrieve(index, wmodel=\"PL2\")\n",
    "\n",
    "# bo1 query expansion\n",
    "bo1 = pt.rewrite.Bo1QueryExpansion(index)\n",
    "bm25_expand = bm25 >> bo1 >> bm25\n",
    "\n",
    "# reranking\n",
    "pipeline = bm25_expand >> (tf ** pl2)\n",
    "\n",
    "# sklearn regressor\n",
    "# rf = RandomForestRegressor(n_estimators=5)\n",
    "# rf_pipe = pipeline >> pt.ltr.apply_learned_model(rf)\n",
    "# rf_pipe.fit(pt_dataset.get_topics(), pt_dataset.get_qrels())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Create the Run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First, we have a short look at the first three topics:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qid</th>\n",
       "      <th>query</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>retrieval system improving effectiveness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>machine learning language identification</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>social media detect self harm</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  qid                                     query\n",
       "0  1   retrieval system improving effectiveness\n",
       "1  2   machine learning language identification\n",
       "2  3   social media detect self harm           "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('First, we have a short look at the first three topics:')\n",
    "pt_dataset.get_topics('text').head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create run\n",
      "Done. Here are the first 10 entries of the run\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qid</th>\n",
       "      <th>docid</th>\n",
       "      <th>docno</th>\n",
       "      <th>score</th>\n",
       "      <th>query</th>\n",
       "      <th>features</th>\n",
       "      <th>rank</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>94858</td>\n",
       "      <td>2004.cikm_conference-2004.47</td>\n",
       "      <td>0.6</td>\n",
       "      <td>retrieval system improving effectiveness</td>\n",
       "      <td>[22.0, 9.600797845496768]</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>125137</td>\n",
       "      <td>1989.ipm_journal-ir0volumeA25A4.2</td>\n",
       "      <td>0.6</td>\n",
       "      <td>retrieval system improving effectiveness</td>\n",
       "      <td>[4.0, 9.618979393386638]</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>84876</td>\n",
       "      <td>2016.ntcir_conference-2016.90</td>\n",
       "      <td>0.8</td>\n",
       "      <td>retrieval system improving effectiveness</td>\n",
       "      <td>[14.0, 8.401517143790793]</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>5868</td>\n",
       "      <td>W05-0704</td>\n",
       "      <td>0.0</td>\n",
       "      <td>retrieval system improving effectiveness</td>\n",
       "      <td>[15.0, 9.282615310408456]</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>125817</td>\n",
       "      <td>2005.ipm_journal-ir0volumeA41A5.11</td>\n",
       "      <td>0.8</td>\n",
       "      <td>retrieval system improving effectiveness</td>\n",
       "      <td>[15.0, 7.8862767550893285]</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>126826</td>\n",
       "      <td>2007.tois_journal-ir0volumeA26A1.4</td>\n",
       "      <td>0.6</td>\n",
       "      <td>retrieval system improving effectiveness</td>\n",
       "      <td>[16.0, 8.818200605377877]</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>94415</td>\n",
       "      <td>2008.cikm_conference-2008.183</td>\n",
       "      <td>0.8</td>\n",
       "      <td>retrieval system improving effectiveness</td>\n",
       "      <td>[18.0, 8.10588972959852]</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>124801</td>\n",
       "      <td>2006.ipm_journal-ir0volumeA42A3.2</td>\n",
       "      <td>0.4</td>\n",
       "      <td>retrieval system improving effectiveness</td>\n",
       "      <td>[17.0, 7.677016688150646]</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>81840</td>\n",
       "      <td>2006.sigirconf_conference-2006.103</td>\n",
       "      <td>0.0</td>\n",
       "      <td>retrieval system improving effectiveness</td>\n",
       "      <td>[8.0, 7.669430024361412]</td>\n",
       "      <td>53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>82472</td>\n",
       "      <td>1998.sigirconf_conference-98.15</td>\n",
       "      <td>0.8</td>\n",
       "      <td>retrieval system improving effectiveness</td>\n",
       "      <td>[12.0, 7.7792092708026255]</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  qid   docid                               docno  score  \\\n",
       "0  1   94858   2004.cikm_conference-2004.47        0.6     \n",
       "1  1   125137  1989.ipm_journal-ir0volumeA25A4.2   0.6     \n",
       "2  1   84876   2016.ntcir_conference-2016.90       0.8     \n",
       "3  1   5868    W05-0704                            0.0     \n",
       "4  1   125817  2005.ipm_journal-ir0volumeA41A5.11  0.8     \n",
       "5  1   126826  2007.tois_journal-ir0volumeA26A1.4  0.6     \n",
       "6  1   94415   2008.cikm_conference-2008.183       0.8     \n",
       "7  1   124801  2006.ipm_journal-ir0volumeA42A3.2   0.4     \n",
       "8  1   81840   2006.sigirconf_conference-2006.103  0.0     \n",
       "9  1   82472   1998.sigirconf_conference-98.15     0.8     \n",
       "\n",
       "                                      query                    features  rank  \n",
       "0  retrieval system improving effectiveness  [22.0, 9.600797845496768]   15    \n",
       "1  retrieval system improving effectiveness  [4.0, 9.618979393386638]    16    \n",
       "2  retrieval system improving effectiveness  [14.0, 8.401517143790793]   2     \n",
       "3  retrieval system improving effectiveness  [15.0, 9.282615310408456]   52    \n",
       "4  retrieval system improving effectiveness  [15.0, 7.8862767550893285]  3     \n",
       "5  retrieval system improving effectiveness  [16.0, 8.818200605377877]   17    \n",
       "6  retrieval system improving effectiveness  [18.0, 8.10588972959852]    4     \n",
       "7  retrieval system improving effectiveness  [17.0, 7.677016688150646]   24    \n",
       "8  retrieval system improving effectiveness  [8.0, 7.669430024361412]    53    \n",
       "9  retrieval system improving effectiveness  [12.0, 7.7792092708026255]  5     "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Create run')\n",
    "run = pipeline(pt_dataset.get_topics('text'))\n",
    "print('Done. Here are the first 10 entries of the run')\n",
    "run.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7: Persist the run file for subsequent evaluations\n",
    "\n",
    "The output of a prototypical retrieval system is a run file. This run file can later (optimally in a different notebook) be statistically evaluated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done. run file is stored under \"../runs/run.txt\".\n"
     ]
    }
   ],
   "source": [
    "persist_and_normalize_run(run, system_name='bm25-knowledge-knights', output_file='../runs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are multiple query fields available: ('text', 'title', 'query', 'description', 'narrative'). To use with pyterrier, provide variant or modify dataframe to add query column.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>ndcg_cut.10</th>\n",
       "      <th>recip_rank</th>\n",
       "      <th>recall_100</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BM25 (Own)</td>\n",
       "      <td>0.905453</td>\n",
       "      <td>0.970588</td>\n",
       "      <td>0.816418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BM 25 (Baseline)</td>\n",
       "      <td>0.374041</td>\n",
       "      <td>0.579877</td>\n",
       "      <td>0.601333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sparse Cross Encoder</td>\n",
       "      <td>0.366460</td>\n",
       "      <td>0.612980</td>\n",
       "      <td>0.601333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RankZephyr</td>\n",
       "      <td>0.347070</td>\n",
       "      <td>0.568413</td>\n",
       "      <td>0.601333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   name  ndcg_cut.10  recip_rank  recall_100\n",
       "0  BM25 (Own)            0.905453     0.970588    0.816418  \n",
       "1  BM 25 (Baseline)      0.374041     0.579877    0.601333  \n",
       "2  Sparse Cross Encoder  0.366460     0.612980    0.601333  \n",
       "3  RankZephyr            0.347070     0.568413    0.601333  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "bm25 = pt.io.read_results('../runs/run.txt')\n",
    "\n",
    "bm25_baseline = tira.pt.from_submission('ir-benchmarks/tira-ir-starter/BM25 (tira-ir-starter-pyterrier)', pt_dataset)\n",
    "sparse_cross_encoder = tira.pt.from_submission('ir-benchmarks/fschlatt/sparse-cross-encoder-4-512', pt_dataset)\n",
    "rank_zephyr = tira.pt.from_submission('workshop-on-open-web-search/fschlatt/rank-zephyr', pt_dataset)\n",
    "\n",
    "pt.Experiment(\n",
    "    [pipeline, bm25_baseline, sparse_cross_encoder, rank_zephyr],\n",
    "    pt_dataset.get_topics(),\n",
    "    pt_dataset.get_qrels(),\n",
    "    [\"ndcg_cut.10\", \"recip_rank\", \"recall_100\"],\n",
    "    names=[\"BM25 (Own)\", \"BM 25 (Baseline)\", \"Sparse Cross Encoder\", \"RankZephyr\"]\n",
    ")\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
